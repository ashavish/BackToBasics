---
title: "Data Preprocessing in R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Outlier Analysis

Outlier analysis might need to be performed as outliers can bias the model. But the action to be taken
 depends on the reason the outliers were introduced in the first place.In some cases it might so happen, that outliers are a different category altogether.

Visualizing could be done either by box plots or histograms.

```{r load}
data(mtcars)
str(mtcars)
```

Lets introduce some outliers in the data first

```{r outlier_induce}
df <- mtcars
outliers <- mtcars[1:3,]
outliers$mpg <- c(45,61,39)
df <- rbind(df,outliers)
data <- df$mpg
hist(data)
boxplot(data)
```

Other than visual identification of outliers, we have some rules to identify mathematically.
A basic one is two exclude anything more than 3 standard deviations from the mean.

Lets create a basic function to do the same.
```{r outlier_1}
outlier_detect <- function(x,mean,sd) {
cut_off_lower <- -3*sd
cut_off_upper <- 3*sd
if (x<cut_off_lower | x> cut_off_upper) {
val <- 1}
else {
val <- 0
}
return(val)
}

mean_val = mean(data)
sd_val = sd(data)
mean_val
sd_val
outlier_flags <- sapply(data,outlier_detect,mean=mean_val,sd=sd_val)
head(data[outlier_flags==1])
length(data[outlier_flags==1])
```

Another one is the Interquartile rule. Anything beyond Q1- 1.5 IQR or Q3 + 1.5 IQR is considered an outlier.

``` {r outlier_2}
outlier_detect_iqr <- function(x,q1,q3,iqr) {
cut_off_lower <- q1 - 1.5*iqr
cut_off_upper <- q3 + 1.5*iqr
if (x<cut_off_lower | x> cut_off_upper) {
val <- 1}
else {
val <- 0
}
return(val)
}

q1 <- quantile(data,0.25)
q3 <- quantile(data,0.75)
iqr <- IQR(data)

q1
q3
iqr

outlier_flags <- sapply(data,outlier_detect_iqr,q1=q1,q3=q3,iqr=iqr)
head(data[outlier_flags==1])
length(data[outlier_flags==1])
```

But normally a univariate/bivariate analysis might not be suffient to detect outliers.This is where we might need to consider Cook's distance.

It measures the impact of a data point on a predicted outcome. Assuming that there are two models, one which includes that particular data point and one without including that data point, cooks distance
computes the squared difference in predictions of all points computed by these two models divided by p (number of coofficients of the model) and MSE (Mean squared error)

![](cook_d.png)

Normally observations which are 4 times the cooks distances, are treated as outliers.

```{r cooks}
mod <- lm(mpg ~ .,data=df)
cooksd <- cooks.distance(mod)
plot(cooksd,pch="*",cex=2,main="Influential Obs")
abline(h = 4*mean(cooksd, na.rm=T), col="red")
```


## Missing value imputations

Both outliers and missing values could require some method of dealing with them.
First we need to identify the missing values. Using the same mtcars dataset, lets first introduce some missing values.

```{r missing}
df <- mtcars
df[c(10,12,20),"mpg"] <- NaN
sum(complete.cases(df))
sum(complete.cases(mtcars))

df[is.na(df),]
```

We can see the three rows which have missing values.
One way to deal with missing values, is to drop them.

```{r missing_drop}
df2 <- na.omit(df)
sum(complete.cases(df2))
nrow(df2)
```

The second method is to impute missing values with some value like mean, median or mode.
Lets try mean imputation here.

```{r impute_mean}
df3 <- df
df3[is.na(df3),"mpg"] <- mean(df[complete.cases(df),"mpg"])
sum(complete.cases(df3))
```

There are other more sophisticated ways to impute values. Like knn imputation.

```{r knn_impute,message=FALSE}
library(caret)
df4 <- df
preProcValues <- preProcess(df4, method = c("knnImpute"),
                            k = 6,
                            knnSummary = mean)
impute_df <- predict(preProcValues, df4,na.action = na.pass)
```

But knn impute also centers and scales the data, which needs to be corrected post imputation.
Lets write a function to correct this.

```{r knn_impute_correct}
center_scale_inverse <- function(x,colno) {
return (x*preProcValues$std[colno] + preProcValues$mean[colno])
}

for (colno in c(1:ncol(df4)))
 {
  impute_df[,colno] <- sapply(impute_df[,colno],center_scale_inverse,colno)
 }

sum(complete.cases(impute_df))

head(impute_df)
```

## Correlations

Next we check for correlations, to see if any of the correlated variables need to be removed.
```{r corr}
descr_corr <-  cor(impute_df)
descr_corr
highly_corr <- findCorrelation(descr_corr, cutoff = .8)
colnames(impute_df)[highly_corr]
```

To know more details including the p-values, we use the Hmisc package
```{r corr_hmisc,message=FALSE}
library("Hmisc")
cor_coeff <- rcorr(as.matrix(impute_df))
cor_coeff$r
cor_coeff$P
```

Lets flatten this matrix by writing a function

```{r corr_flatten}
flatten_corr <- function(cormat,pmat)
{
  ut <- upper.tri(cormat)
  df <- data.frame(
  row = rownames(cormat)[row(cormat)][ut],
  col = rownames(cormat)[col(cormat)][ut],
  cor = cormat[ut],
  p = pmat[ut]
  )
  return(df)
}

flat_cor <- flatten_corr(cor_coeff$r,cor_coeff$P)
head(flat_cor)
```

For purely linear combinations, caret has a method called findLinearCombos

```{r linearCombo}
comboInfo <- findLinearCombos(impute_df)
```

## Other Preprocessing

Many preprocessing requirements is dependent on the type of variables. Lets divide the columns into continuous and categorical variables.
```{r factor_numeric}
data("ToothGrowth")
df <- ToothGrowth

factor_variables <- lapply(df,is.factor)
numeric_variables <- lapply(df,is.numeric)

factor_variables
numeric_variables
```


One important aspect of preprocessing is to create dummy variables for factor variables. Library caret can create dummy variables for all factor variables in a dataset


```{r dummy_test}
df <- ToothGrowth
dummies <- dummyVars(~ ., data = df)
df_encoded <- predict(dummies, df)
head(df_encoded)
```

Other commonly used preprocessing methods is to center and scale. Lets split the data into a train and test.
Again caret library can be used to partition the data. We partition the data and then use the train split to train the preprocess method.


```{r preprocess_module}
set.seed(2341)
data(ToothGrowth)
df_part <- ToothGrowth
trainIndex <- createDataPartition(df_part$supp, p = 0.8, list = FALSE)
train_df <- df_part[trainIndex, ]
test_df <- df_part[-trainIndex, ]
preProcValues <- preProcess(train_df, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, train_df)
testTransformed <- predict(preProcValues, test_df)
head(trainTransformed)
head(testTransformed)
```


